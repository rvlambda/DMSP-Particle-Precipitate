{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "import datetime\n",
    "from os.path import isfile, join\n",
    "from sys import getsizeof\n",
    "import glob\n",
    "\n",
    "from random import *\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from pickle import dump\n",
    "import joblib\n",
    "\n",
    "# from keras import backend as K\n",
    "import tensorflow.keras as k\n",
    "from tensorflow.keras import utils, models, layers, optimizers\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "df = pd.read_csv(\"D:/Revi/Courses/TMLC/Projects/2. Deep Learning/4. DMSP Particle Precipitate/Dataset/AI_Ready_DMSP_Data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Datetimes',\n",
       " 'SC_AACGM_LAT',\n",
       " 'SC_AACGM_LTIME',\n",
       " 'ELE_TOTAL_ENERGY_FLUX',\n",
       " 'ELE_TOTAL_ENERGY_FLUX_STD',\n",
       " 'ELE_AVG_ENERGY',\n",
       " 'ELE_AVG_ENERGY_STD',\n",
       " 'ID_SC',\n",
       " 'sin_ut',\n",
       " 'cos_ut',\n",
       " 'sin_doy',\n",
       " 'cos_doy',\n",
       " 'sin_SC_AACGM_LTIME',\n",
       " 'cos_SC_AACGM_LTIME',\n",
       " 'F107',\n",
       " 'Bz',\n",
       " 'By',\n",
       " 'Bx',\n",
       " 'AE',\n",
       " 'AL',\n",
       " 'AU',\n",
       " 'SymH',\n",
       " 'PC',\n",
       " 'vsw',\n",
       " 'vx',\n",
       " 'psw',\n",
       " 'borovsky',\n",
       " 'newell',\n",
       " 'F107_6hr',\n",
       " 'Bz_6hr',\n",
       " 'By_6hr',\n",
       " 'Bx_6hr',\n",
       " 'AE_6hr',\n",
       " 'AL_6hr',\n",
       " 'AU_6hr',\n",
       " 'SymH_6hr',\n",
       " 'PC_6hr',\n",
       " 'vsw_6hr',\n",
       " 'vx_6hr',\n",
       " 'psw_6hr',\n",
       " 'borovsky_6hr',\n",
       " 'newell_6hr',\n",
       " 'F107_5hr',\n",
       " 'Bz_5hr',\n",
       " 'By_5hr',\n",
       " 'Bx_5hr',\n",
       " 'AE_5hr',\n",
       " 'AL_5hr',\n",
       " 'AU_5hr',\n",
       " 'SymH_5hr',\n",
       " 'PC_5hr',\n",
       " 'vsw_5hr',\n",
       " 'vx_5hr',\n",
       " 'psw_5hr',\n",
       " 'borovsky_5hr',\n",
       " 'newell_5hr',\n",
       " 'F107_3hr',\n",
       " 'Bz_3hr',\n",
       " 'By_3hr',\n",
       " 'Bx_3hr',\n",
       " 'AE_3hr',\n",
       " 'AL_3hr',\n",
       " 'AU_3hr',\n",
       " 'SymH_3hr',\n",
       " 'PC_3hr',\n",
       " 'vsw_3hr',\n",
       " 'vx_3hr',\n",
       " 'psw_3hr',\n",
       " 'borovsky_3hr',\n",
       " 'newell_3hr',\n",
       " 'F107_1hr',\n",
       " 'Bz_1hr',\n",
       " 'By_1hr',\n",
       " 'Bx_1hr',\n",
       " 'AE_1hr',\n",
       " 'AL_1hr',\n",
       " 'AU_1hr',\n",
       " 'SymH_1hr',\n",
       " 'PC_1hr',\n",
       " 'vsw_1hr',\n",
       " 'vx_1hr',\n",
       " 'psw_1hr',\n",
       " 'borovsky_1hr',\n",
       " 'newell_1hr',\n",
       " 'F107_45min',\n",
       " 'Bz_45min',\n",
       " 'By_45min',\n",
       " 'Bx_45min',\n",
       " 'AE_45min',\n",
       " 'AL_45min',\n",
       " 'AU_45min',\n",
       " 'SymH_45min',\n",
       " 'PC_45min',\n",
       " 'vsw_45min',\n",
       " 'vx_45min',\n",
       " 'psw_45min',\n",
       " 'borovsky_45min',\n",
       " 'newell_45min',\n",
       " 'F107_30min',\n",
       " 'Bz_30min',\n",
       " 'By_30min',\n",
       " 'Bx_30min',\n",
       " 'AE_30min',\n",
       " 'AL_30min',\n",
       " 'AU_30min',\n",
       " 'SymH_30min',\n",
       " 'PC_30min',\n",
       " 'vsw_30min',\n",
       " 'vx_30min',\n",
       " 'psw_30min',\n",
       " 'borovsky_30min',\n",
       " 'newell_30min',\n",
       " 'F107_15min',\n",
       " 'Bz_15min',\n",
       " 'By_15min',\n",
       " 'Bx_15min',\n",
       " 'AE_15min',\n",
       " 'AL_15min',\n",
       " 'AU_15min',\n",
       " 'SymH_15min',\n",
       " 'PC_15min',\n",
       " 'vsw_15min',\n",
       " 'vx_15min',\n",
       " 'psw_15min',\n",
       " 'borovsky_15min',\n",
       " 'newell_15min',\n",
       " 'F107_10min',\n",
       " 'Bz_10min',\n",
       " 'By_10min',\n",
       " 'Bx_10min',\n",
       " 'AE_10min',\n",
       " 'AL_10min',\n",
       " 'AU_10min',\n",
       " 'SymH_10min',\n",
       " 'PC_10min',\n",
       " 'vsw_10min',\n",
       " 'vx_10min',\n",
       " 'psw_10min',\n",
       " 'borovsky_10min',\n",
       " 'newell_10min',\n",
       " 'F107_5min',\n",
       " 'Bz_5min',\n",
       " 'By_5min',\n",
       " 'Bx_5min',\n",
       " 'AE_5min',\n",
       " 'AL_5min',\n",
       " 'AU_5min',\n",
       " 'SymH_5min',\n",
       " 'PC_5min',\n",
       " 'vsw_5min',\n",
       " 'vx_5min',\n",
       " 'psw_5min',\n",
       " 'borovsky_5min',\n",
       " 'newell_5min']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_index('Datetimes')\n",
    "df.index = pd.to_datetime(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SC_AACGM_LTIME', 'ELE_TOTAL_ENERGY_FLUX_STD', 'ELE_AVG_ENERGY', 'ELE_AVG_ENERGY_STD', 'AE', 'vx', 'borovsky', 'newell', 'F107_6hr', 'AE_6hr', 'vx_6hr', 'borovsky_6hr', 'newell_6hr', 'F107_5hr', 'Bz_5hr', 'By_5hr', 'Bx_5hr', 'AE_5hr', 'AL_5hr', 'AU_5hr', 'SymH_5hr', 'PC_5hr', 'vsw_5hr', 'vx_5hr', 'psw_5hr', 'borovsky_5hr', 'newell_5hr', 'F107_3hr', 'AE_3hr', 'vx_3hr', 'borovsky_3hr', 'newell_3hr', 'F107_1hr', 'AE_1hr', 'vx_1hr', 'borovsky_1hr', 'newell_1hr', 'F107_45min', 'AE_45min', 'vx_45min', 'borovsky_45min', 'newell_45min', 'F107_30min', 'AE_30min', 'vx_30min', 'borovsky_30min', 'newell_30min', 'F107_15min', 'Bz_15min', 'By_15min', 'Bx_15min', 'AE_15min', 'AL_15min', 'AU_15min', 'SymH_15min', 'PC_15min', 'vsw_15min', 'vx_15min', 'psw_15min', 'borovsky_15min', 'newell_15min', 'F107_10min', 'AE_10min', 'vx_10min', 'borovsky_10min', 'newell_10min', 'F107_5min', 'Bz_5min', 'By_5min', 'Bx_5min', 'AE_5min', 'AL_5min', 'AU_5min', 'SymH_5min', 'PC_5min', 'vsw_5min', 'vx_5min', 'psw_5min', 'borovsky_5min', 'newell_5min']\n"
     ]
    }
   ],
   "source": [
    "# Choose columns to drop\n",
    "cols_to_drop = [c for c in df.columns if ('1min' in c) | ('3min' in c) | ('4min' in c) | ('_5min'in c) | ('15min' in c)| ('5hr' in c) | ('F107_' in c) | ('vx' in c) | ('AE' in c) | ('newell' in c) | ('borovsky' in c) | ('STD' in c) | ('AVG' in c) | ('SC_AACGM_LTIME'==c)]\n",
    "\n",
    "print(cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SC_AACGM_LAT',\n",
       " 'ELE_TOTAL_ENERGY_FLUX',\n",
       " 'ID_SC',\n",
       " 'sin_ut',\n",
       " 'cos_ut',\n",
       " 'sin_doy',\n",
       " 'cos_doy',\n",
       " 'sin_SC_AACGM_LTIME',\n",
       " 'cos_SC_AACGM_LTIME',\n",
       " 'F107',\n",
       " 'Bz',\n",
       " 'By',\n",
       " 'Bx',\n",
       " 'AL',\n",
       " 'AU',\n",
       " 'SymH',\n",
       " 'PC',\n",
       " 'vsw',\n",
       " 'psw',\n",
       " 'Bz_6hr',\n",
       " 'By_6hr',\n",
       " 'Bx_6hr',\n",
       " 'AL_6hr',\n",
       " 'AU_6hr',\n",
       " 'SymH_6hr',\n",
       " 'PC_6hr',\n",
       " 'vsw_6hr',\n",
       " 'psw_6hr',\n",
       " 'Bz_3hr',\n",
       " 'By_3hr',\n",
       " 'Bx_3hr',\n",
       " 'AL_3hr',\n",
       " 'AU_3hr',\n",
       " 'SymH_3hr',\n",
       " 'PC_3hr',\n",
       " 'vsw_3hr',\n",
       " 'psw_3hr',\n",
       " 'Bz_1hr',\n",
       " 'By_1hr',\n",
       " 'Bx_1hr',\n",
       " 'AL_1hr',\n",
       " 'AU_1hr',\n",
       " 'SymH_1hr',\n",
       " 'PC_1hr',\n",
       " 'vsw_1hr',\n",
       " 'psw_1hr',\n",
       " 'Bz_45min',\n",
       " 'By_45min',\n",
       " 'Bx_45min',\n",
       " 'AL_45min',\n",
       " 'AU_45min',\n",
       " 'SymH_45min',\n",
       " 'PC_45min',\n",
       " 'vsw_45min',\n",
       " 'psw_45min',\n",
       " 'Bz_30min',\n",
       " 'By_30min',\n",
       " 'Bx_30min',\n",
       " 'AL_30min',\n",
       " 'AU_30min',\n",
       " 'SymH_30min',\n",
       " 'PC_30min',\n",
       " 'vsw_30min',\n",
       " 'psw_30min',\n",
       " 'Bz_10min',\n",
       " 'By_10min',\n",
       " 'Bx_10min',\n",
       " 'AL_10min',\n",
       " 'AU_10min',\n",
       " 'SymH_10min',\n",
       " 'PC_10min',\n",
       " 'vsw_10min',\n",
       " 'psw_10min']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List the new set of columns\n",
    "df.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of input features total = 73\n"
     ]
    }
   ],
   "source": [
    "print('number of input features total = {}'.format(df.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the threshold above which to remove energy flux values\n",
    "eflux_threshold_quantile = 99.995\n",
    "eflux_threshold = .99995\n",
    "\n",
    "# Define the conversion factor from eV to erg\n",
    "erg_eV_factor = (1.60218e-12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True, ...,  True,  True,  True])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold_drop_val = np.percentile(df['ELE_TOTAL_ENERGY_FLUX'],eflux_threshold_quantile)\n",
    "df['ELE_TOTAL_ENERGY_FLUX'].values < threshold_drop_val "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1945887, 73)\n",
      "... dropping rows where total electron energy flux is greater than 7.3673E+13 [eV/cm2/s/ster]\n",
      "--------> number dropped = 98\n",
      "(1945789, 73)\n"
     ]
    }
   ],
   "source": [
    "# Remove outliers\n",
    "df_LTIMEs = [[]]\n",
    "df_STDs = [[]]\n",
    "print(df.shape)\n",
    "threshold_drop_val = np.percentile(df['ELE_TOTAL_ENERGY_FLUX'],eflux_threshold_quantile)\n",
    "print('... dropping rows where total electron energy flux is greater than {:.4E} [eV/cm2/s/ster]'.format(threshold_drop_val))\n",
    "print('--------> number dropped = {}'.format(len( np.argwhere(df['ELE_TOTAL_ENERGY_FLUX'].values > threshold_drop_val) )))\n",
    "#df_LTIMEs = df_LTIMEs[ (df['ELE_TOTAL_ENERGY_FLUX'].values < threshold_drop_val) ]\n",
    "#df_STDs = df_STDs[ (df['ELE_TOTAL_ENERGY_FLUX'].values < threshold_drop_val) ]\n",
    "df = df[ (df['ELE_TOTAL_ENERGY_FLUX'].values < threshold_drop_val) ]\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 73)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.sample(1000)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation data shape = (26, 73)\n",
      "train data shape = (974, 73)\n",
      "NOTE: we have called the withheld data *validation* data here\n"
     ]
    }
   ],
   "source": [
    "# Separate training and testing data\n",
    "mask_val = [(df.index.year == 2010) & (df['ID_SC'].values==16)]\n",
    "df_val = df[mask_val[0]].copy(deep=True)\n",
    "df_train = df.copy(deep=True).drop( df.index[mask_val[0]])\n",
    "print('validation data shape = {}'.format(df_val.shape))\n",
    "print('train data shape = {}'.format(df_train.shape))\n",
    "print('NOTE: we have called the withheld data *validation* data here')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct X and y\n",
    "feature_cols = [c for c in df.columns if not 'ELE' in c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = df_val[feature_cols].copy(deep=True)\n",
    "y_val = df_val['ELE_TOTAL_ENERGY_FLUX'].copy(deep=True)\n",
    "X_train = df_train[feature_cols].copy(deep=True)\n",
    "y_train = df_train['ELE_TOTAL_ENERGY_FLUX'].copy(deep=True)\n",
    "scaler_X = RobustScaler()\n",
    "scaler_X = scaler_X.fit(X_train.values)\n",
    "X_val_scaled = scaler_X.transform(X_val.values)\n",
    "X_train_scaled = scaler_X.transform(X_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: removing the steradian from the DMSP data BEFORE training\n"
     ]
    }
   ],
   "source": [
    "# The units of the energy flux are eV/cm2/s/ster\n",
    "#   --> To remove the 'steradian' multiply by PI (see: https://agupubs.onlinelibrary.wiley.com/doi/epdf/10.1002/2016JA023339)\n",
    "y_train = y_train * np.pi\n",
    "y_val = y_val * np.pi\n",
    "print('NOTE: removing the steradian from the DMSP data BEFORE training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create targets in erg/cm2/s units to explore target in different units\n",
    "y_train_erg = y_train.copy(deep=True) * (1.60218e-12)\n",
    "y_val_erg = y_val.copy(deep=True) * (1.60218e-12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create targets in log10(eV/cm2/s) units to explore target in different units\n",
    "y_train[y_train == 0] = 0.00001\n",
    "y_val[y_val == 0] = 0.00001\n",
    "y_train_log = np.log10(y_train.copy(deep=True))\n",
    "y_val_log = np.log10(y_val.copy(deep=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a neural network model and train\n",
    "# define base model\n",
    "def baseline_model():\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(13, input_dim=72, kernel_initializer='normal', activation='relu'))\n",
    "\tmodel.add(Dense(1, kernel_initializer='normal'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "def larger_model():\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(13, input_dim=72, kernel_initializer='normal', activation='relu'))\n",
    "\tmodel.add(Dense(6, kernel_initializer='normal', activation='relu'))\n",
    "\tmodel.add(Dense(1, kernel_initializer='normal'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define wider model\n",
    "def wider_model():\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(20, input_dim=72, kernel_initializer='normal', activation='relu'))\n",
    "\tmodel.add(Dense(1, kernel_initializer='normal'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Revi\\AppData\\Local\\Temp/ipykernel_17384/3031099890.py:2: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  estimator = KerasRegressor(build_fn=baseline_model, epochs=100, batch_size=5, verbose=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: -129138374515702989062144.00 (104434242413778824069120.00) MSE\n"
     ]
    }
   ],
   "source": [
    "# evaluate model\n",
    "estimator = KerasRegressor(build_fn=baseline_model, epochs=100, batch_size=5, verbose=0)\n",
    "kfold = KFold(n_splits=10)\n",
    "results = cross_val_score(estimator, X_train, y_train, cv=kfold)\n",
    "print(\"Baseline: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Revi\\AppData\\Local\\Temp/ipykernel_17384/252686729.py:5: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  estimators.append(('mlp', KerasRegressor(build_fn=larger_model, epochs=50, batch_size=5, verbose=0)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Larger: -129139144406059284692992.00 (104434460836231588610048.00) MSE\n"
     ]
    }
   ],
   "source": [
    "#Boosting the performance by standardizing the inputs\n",
    "# evaluate model with standardized dataset\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasRegressor(build_fn=larger_model, epochs=50, batch_size=5, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = KFold(n_splits=10)\n",
    "results = cross_val_score(pipeline, X_train, y_train, cv=kfold)\n",
    "print(\"Larger: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Revi\\AppData\\Local\\Temp/ipykernel_17384/2925588626.py:3: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  estimators.append(('mlp', KerasRegressor(build_fn=wider_model, epochs=100, batch_size=5, verbose=0)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wider: -129139150936278741024768.00 (104434457654033125474304.00) MSE\n"
     ]
    }
   ],
   "source": [
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasRegressor(build_fn=wider_model, epochs=100, batch_size=5, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = KFold(n_splits=10)\n",
    "results = cross_val_score(pipeline, X_train, y_train, cv=kfold)\n",
    "print(\"Wider: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_eval(obs,pred):\n",
    "\n",
    "    import numpy as np\n",
    "    from sklearn import linear_model\n",
    "    \n",
    "    if obs.shape != pred.shape:\n",
    "        print('flattening arrays')\n",
    "        obs = obs.flatten()\n",
    "        pred = pred.flatten()\n",
    "        \n",
    "    # Handle NaNs and Infs - ADDED RMM 8.3.2020 to handle NaNs and Infs\n",
    "    nan_pred_mask = np.isnan(pred) | np.isinf(pred)\n",
    "    if np.any(nan_pred_mask):\n",
    "        not_nan_pred_mask = ~nan_pred_mask\n",
    "        pred = pred[not_nan_pred_mask]\n",
    "        obs = obs[not_nan_pred_mask]\n",
    "        \n",
    "#     from sklearn import metrics\n",
    "    MAE = 1./len(obs) * (np.sum( np.abs(obs - pred) ))\n",
    "#     MAE = metrics.mean_absolute_error(obs,pred)\n",
    "    MSE = 1./len(obs) * (np.sum( (obs - pred)**2 ))\n",
    "#     MSE = metrics.mean_squared_error(obs,pred)\n",
    "    RMSE = np.sqrt(MSE)\n",
    "    RAE = (np.sum( np.abs(obs - pred) )) / (np.sum( np.abs(obs - np.mean(obs)) ))\n",
    "    RSE = (np.sum( (obs - pred)**2 )) / (np.sum( (obs - np.mean(obs))**2 ))\n",
    "    RSQ = 1-RSE\n",
    "#     RSQ = metrics.r2_score(obs,pred)\n",
    "#     NOTE: Prediction efficiency and R-squared are the same thing\n",
    "#     PE = 1 - ( (np.sum(pred - obs))**2 ) / ( (np.sum(obs - np.nanmean(obs)))**2 )\n",
    "    #regr = linear_model.LinearRegression()\n",
    "    #regr.fit(obs.reshape(-1, 1),pred)\n",
    "    #lin_slope = regr.coef_[0]\n",
    "   # lin_intercept = regr.intercept_\n",
    "    \n",
    "\n",
    "    print('Evaluation: \\n')\n",
    "    print('   MAE  = {}'.format(MAE))\n",
    "    print('   MSE  = {}'.format(MSE))\n",
    "    print('   RMSE = {}'.format(RMSE))\n",
    "    print('   RAE  = {}'.format(RAE))\n",
    "    print('   RSE  = {}'.format(RSE))\n",
    "    print('   RSQ  = {}'.format(RSQ))\n",
    "    return MAE, MSE, RMSE, RAE, RSE, RSQ\n",
    "#     print('   PE  = {}'.format(PE))\n",
    "   # print('   linear slope  = {}'.format(lin_slope))\n",
    "   # print('   linear intercept  = {}'.format(lin_intercept))\n",
    "    \n",
    "   # return MAE, MSE, RMSE, RAE, RSE, RSQ  #, lin_slope, lin_intercept\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26,)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26,)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation: \n",
      "\n",
      "   MAE  = 79790651345.9144\n",
      "   MSE  = 4.043879503910716e+22\n",
      "   RMSE = 201093995532.2067\n",
      "   RAE  = 0.7242210009086224\n",
      "   RSE  = 1.1868543787612056\n",
      "   RSQ  = -0.1868543787612056\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(79790651345.9144,\n",
       " 4.043879503910716e+22,\n",
       " 201093995532.2067,\n",
       " 0.7242210009086224,\n",
       " 1.1868543787612056,\n",
       " -0.1868543787612056)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply model to predict\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_preds = pipeline.predict(X_val)\n",
    "model_eval(y_val,y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation: \n",
      "\n",
      "   MAE  = 1.4538664844072735\n",
      "   MSE  = 2.7608528724484946\n",
      "   RMSE = 1.661581437200264\n",
      "   RAE  = 0.9955320085930222\n",
      "   RSE  = 1.145336666099887\n",
      "   RSQ  = -0.14533666609988694\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.4538664844072735,\n",
       " 2.7608528724484946,\n",
       " 1.661581437200264,\n",
       " 0.9955320085930222,\n",
       " 1.145336666099887,\n",
       " -0.14533666609988694)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Executing the model on scaLed model\n",
    "pipeline.fit(X_train_scaled, y_train_log)\n",
    "y_preds = pipeline.predict(X_val_scaled)\n",
    "model_eval(y_val_log,y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PreProcessData(df):\n",
    "    \n",
    "    # Remove outliers\n",
    "    threshold_drop_val = np.percentile(df['ELE_TOTAL_ENERGY_FLUX'],eflux_threshold_quantile)\n",
    "    print('... dropping rows where total electron energy flux is greater than {:.4E} [eV/cm2/s/ster]'.format(threshold_drop_val))\n",
    "    print('--------> number dropped = {}'.format(len( np.argwhere(df['ELE_TOTAL_ENERGY_FLUX'].values > threshold_drop_val) )))\n",
    "    df = df[ (df['ELE_TOTAL_ENERGY_FLUX'].values < threshold_drop_val) ]\n",
    "    print(df.shape)\n",
    "\n",
    "\n",
    "    # Separate training and testing data\n",
    "    mask_val = [(df.index.year == 2010) & (df['ID_SC'].values==16)]\n",
    "    df_val = df[mask_val[0]].copy(deep=True)\n",
    "    df_train = df.copy(deep=True).drop( df.index[mask_val[0]])\n",
    "    print('validation data shape = {}'.format(df_val.shape))\n",
    "    print('train data shape = {}'.format(df_train.shape))\n",
    "    print('NOTE: we have called the withheld data *validation* data here')\n",
    "\n",
    "\n",
    "    # Scale and Transform inputs\n",
    "    X_val = df_val[feature_cols].copy(deep=True)\n",
    "    y_val = df_val['ELE_TOTAL_ENERGY_FLUX'].copy(deep=True)\n",
    "    X_train = df_train[feature_cols].copy(deep=True)\n",
    "    y_train = df_train['ELE_TOTAL_ENERGY_FLUX'].copy(deep=True)\n",
    "    scaler_X = RobustScaler()\n",
    "    scaler_X = scaler_X.fit(X_train.values)\n",
    "    X_val_scaled = scaler_X.transform(X_val.values)\n",
    "    X_train_scaled = scaler_X.transform(X_train.values)\n",
    "\n",
    "    # Scale and Transform labels\n",
    "    # The units of the energy flux are eV/cm2/s/ster\n",
    "    #   --> To remove the 'steradian' multiply by PI (see: https://agupubs.onlinelibrary.wiley.com/doi/epdf/10.1002/2016JA023339)\n",
    "    y_train = y_train * np.pi\n",
    "    y_val = y_val * np.pi\n",
    "    print('NOTE: removing the steradian from the DMSP data BEFORE training')\n",
    "    # Create targets in erg/cm2/s units to explore target in different units\n",
    "    y_train_erg = y_train.copy(deep=True) * (1.60218e-12)\n",
    "    y_val_erg = y_val.copy(deep=True) * (1.60218e-12)\n",
    "    # Create targets in log10(eV/cm2/s) units to explore target in different units\n",
    "    y_train[y_train == 0] = 0.00001\n",
    "    y_val[y_val == 0] = 0.00001\n",
    "    y_train_log = np.log10(y_train.copy(deep=True))\n",
    "    y_val_log = np.log10(y_val.copy(deep=True))\n",
    "    \n",
    "    return X_train_scaled, X_val_scaled,y_train_log, y_val_log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateAndTrainModel():\n",
    "    # Defining Checkpoints fr the model \n",
    "    checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \n",
    "    file_name = os.path.join(\"./model/\",checkpoint_name)\n",
    "    checkpoint = ModelCheckpoint(file_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n",
    "    callback_ES = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "    callback_TB = tf.keras.callbacks.TensorBoard(log_dir='./logs')\n",
    "    callbacks_list = [checkpoint,callback_ES,callback_TB]\n",
    "\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(20, input_dim=72, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_error', optimizer='sgd')\n",
    "    #estimators = []\n",
    "    #estimators.append(('standardize', StandardScaler()))\n",
    "    #estimators.append(('mlp', KerasRegressor(build_fn=wider_model, epochs=100, batch_size=5, verbose=0, callbacks=callbacks_list)))\n",
    "    #pipeline = Pipeline(estimators)\n",
    "    #return pipeline\n",
    "    history = model.fit(X_train_scaled,y_train_log, epochs=100,validation_data=(X_val_scaled,y_val_log),callbacks=callbacks_list)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainModel(pipeline,X_train_scaled,X_val_scaled,y_val_log):\n",
    "    # Executing the model on scaLed model\n",
    "    pipeline.fit(X_train_scaled, y_train_log)\n",
    "    y_preds = pipeline.predict(X_val_scaled)\n",
    "    return y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ValidateModel(y_val_log,y_preds):\n",
    "    # Validate the model\n",
    "    model_eval(y_val_log,y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... dropping rows where total electron energy flux is greater than 1.2880E+13 [eV/cm2/s/ster]\n",
      "--------> number dropped = 98\n",
      "(1945691, 73)\n",
      "validation data shape = (55208, 73)\n",
      "train data shape = (1836963, 73)\n",
      "NOTE: we have called the withheld data *validation* data here\n",
      "NOTE: removing the steradian from the DMSP data BEFORE training\n"
     ]
    }
   ],
   "source": [
    "#Training the model for entire dataset\n",
    "\n",
    "df = df_tmp.copy()\n",
    "X_train_scaled, X_val_scaled,y_train_log, y_val_log = PreProcessData(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_log.dtype\n",
    "actual = np.array(y_val_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55208,)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = y_preds.reshape(55208,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 1.        , 1.        , ..., 0.99999976, 0.99999976,\n",
       "       0.99999976], dtype=float32)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "57400/57406 [============================>.] - ETA: 0s - loss: 75.7840\n",
      "Epoch 1: val_loss improved from inf to 71.51799, saving model to ./model\\Weights-001--71.51799.hdf5\n",
      "57406/57406 [==============================] - 155s 3ms/step - loss: 75.7842 - val_loss: 71.5180\n",
      "Epoch 2/100\n",
      "57405/57406 [============================>.] - ETA: 0s - loss: 75.7780\n",
      "Epoch 2: val_loss improved from 71.51799 to 71.51788, saving model to ./model\\Weights-002--71.51788.hdf5\n",
      "57406/57406 [==============================] - 157s 3ms/step - loss: 75.7780 - val_loss: 71.5179\n",
      "Epoch 3/100\n",
      "57395/57406 [============================>.] - ETA: 0s - loss: 75.7782\n",
      "Epoch 3: val_loss improved from 71.51788 to 71.51785, saving model to ./model\\Weights-003--71.51785.hdf5\n",
      "57406/57406 [==============================] - 152s 3ms/step - loss: 75.7780 - val_loss: 71.5178\n",
      "Epoch 4/100\n",
      "57390/57406 [============================>.] - ETA: 0s - loss: 75.7776\n",
      "Epoch 4: val_loss improved from 71.51785 to 71.51783, saving model to ./model\\Weights-004--71.51783.hdf5\n",
      "57406/57406 [==============================] - 150s 3ms/step - loss: 75.7777 - val_loss: 71.5178\n",
      "Epoch 5/100\n",
      "57403/57406 [============================>.] - ETA: 0s - loss: 75.7777\n",
      "Epoch 5: val_loss improved from 71.51783 to 71.51782, saving model to ./model\\Weights-005--71.51782.hdf5\n",
      "57406/57406 [==============================] - 158s 3ms/step - loss: 75.7778 - val_loss: 71.5178\n",
      "Epoch 6/100\n",
      "57399/57406 [============================>.] - ETA: 0s - loss: 75.7786\n",
      "Epoch 6: val_loss improved from 71.51782 to 71.51781, saving model to ./model\\Weights-006--71.51781.hdf5\n",
      "57406/57406 [==============================] - 159s 3ms/step - loss: 75.7786 - val_loss: 71.5178\n",
      "Epoch 7/100\n",
      "57393/57406 [============================>.] - ETA: 0s - loss: 75.7779\n",
      "Epoch 7: val_loss improved from 71.51781 to 71.51781, saving model to ./model\\Weights-007--71.51781.hdf5\n",
      "57406/57406 [==============================] - 160s 3ms/step - loss: 75.7778 - val_loss: 71.5178\n",
      "1726/1726 [==============================] - 3s 2ms/step - loss: 71.5178\n",
      "71.51780700683594\n"
     ]
    }
   ],
   "source": [
    "# Create a model\n",
    "model = CreateAndTrainModel()\n",
    "\n",
    "\n",
    "# Train the model\n",
    "y_preds = model.predict(X_val_scaled)\n",
    "\n",
    "# Validate the model\n",
    "#ValidateModel(y_val_log,y_preds)\n",
    "\n",
    "mse_test = model.evaluate(X_val_scaled,y_val_log)\n",
    "print(mse_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55208,)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val_log.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55208, 1)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_preds.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation: \n",
      "\n",
      "   MAE  = 8.317060460472954\n",
      "   MSE  = 71.51790967915615\n",
      "   RMSE = 8.456826217864249\n",
      "   RAE  = 6.009720632038825\n",
      "   RSE  = 30.505655717542275\n",
      "   RSQ  = -29.505655717542275\n"
     ]
    }
   ],
   "source": [
    "ValidateModel(actual,predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val_scaled.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... dropping rows where total electron energy flux is greater than 4.2938E+12 [eV/cm2/s/ster]\n",
      "--------> number dropped = 1\n",
      "(999, 73)\n",
      "validation data shape = (27, 73)\n",
      "train data shape = (972, 73)\n",
      "NOTE: we have called the withheld data *validation* data here\n",
      "NOTE: removing the steradian from the DMSP data BEFORE training\n",
      "10\n",
      "[False False False False False False False False False False False False\n",
      " False False False False False False False False  True  True False False\n",
      " False  True  True  True  True False False False False False False False\n",
      " False False False False False False False False  True False  True False\n",
      "  True False False False False False False False False False False False\n",
      " False False False  True False False False False False False False False]\n",
      "[17 63 10 26  6 19 21 15 62 11 20 33 35  2 34 45 18 13 28 36  1  1  9  7\n",
      "  3  1  1  1  1 27  8 23 52 38 37 48 16 12 42 44 56 39 61 30  1 40  1 55\n",
      "  1 60 49 59 43 25 24 31 51 22 53 57 58 50 46  1  5 47 14 41 54 29  4 32]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df = df_tmp.sample(1000)\n",
    "X_train_scaled, X_val_scaled,y_train_log, y_val_log = PreProcessData(df)\n",
    "\n",
    "#convert y values to categorical values\n",
    "lab = preprocessing.LabelEncoder()\n",
    "y_transformed = lab.fit_transform(y_val_log)\n",
    "\n",
    "model1 = LogisticRegression()\n",
    "rfe = RFE(model1,n_features_to_select=10)\n",
    "fit = rfe.fit(X_val_scaled, y_transformed)\n",
    "#print(\"Num Features: %d\") % fit.n_features_\n",
    "#print(\"Selected Features: %s\") % fit.support_\n",
    "#print(\"Feature Ranking: %s\") % fit.ranking_\n",
    "print(fit.n_features_)\n",
    "print(fit.support_)\n",
    "print(fit.ranking_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['SC_AACGM_LAT', 'ELE_TOTAL_ENERGY_FLUX', 'ID_SC', 'sin_ut', 'cos_ut',\n",
       "       'sin_doy', 'cos_doy', 'sin_SC_AACGM_LTIME', 'cos_SC_AACGM_LTIME',\n",
       "       'F107', 'Bz', 'By', 'Bx', 'AL', 'AU', 'SymH', 'PC', 'vsw', 'psw',\n",
       "       'Bz_6hr', 'By_6hr', 'Bx_6hr', 'AL_6hr', 'AU_6hr', 'SymH_6hr', 'PC_6hr',\n",
       "       'vsw_6hr', 'psw_6hr', 'Bz_3hr', 'By_3hr', 'Bx_3hr', 'AL_3hr', 'AU_3hr',\n",
       "       'SymH_3hr', 'PC_3hr', 'vsw_3hr', 'psw_3hr', 'Bz_1hr', 'By_1hr',\n",
       "       'Bx_1hr', 'AL_1hr', 'AU_1hr', 'SymH_1hr', 'PC_1hr', 'vsw_1hr',\n",
       "       'psw_1hr', 'Bz_45min', 'By_45min', 'Bx_45min', 'AL_45min', 'AU_45min',\n",
       "       'SymH_45min', 'PC_45min', 'vsw_45min', 'psw_45min', 'Bz_30min',\n",
       "       'By_30min', 'Bx_30min', 'AL_30min', 'AU_30min', 'SymH_30min',\n",
       "       'PC_30min', 'vsw_30min', 'psw_30min', 'Bz_10min', 'By_10min',\n",
       "       'Bx_10min', 'AL_10min', 'AU_10min', 'SymH_10min', 'PC_10min',\n",
       "       'vsw_10min', 'psw_10min'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit.support_.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['SC_AACGM_LAT', 'ID_SC', 'sin_ut', 'cos_ut', 'sin_doy', 'cos_doy',\n",
       "       'sin_SC_AACGM_LTIME', 'cos_SC_AACGM_LTIME', 'F107', 'Bz', 'By', 'Bx',\n",
       "       'AL', 'AU', 'SymH', 'PC', 'vsw', 'psw', 'Bz_6hr', 'By_6hr', 'Bx_6hr',\n",
       "       'AL_6hr', 'AU_6hr', 'SymH_6hr', 'PC_6hr', 'vsw_6hr', 'psw_6hr',\n",
       "       'Bz_3hr', 'By_3hr', 'Bx_3hr', 'AL_3hr', 'AU_3hr', 'SymH_3hr', 'PC_3hr',\n",
       "       'vsw_3hr', 'psw_3hr', 'Bz_1hr', 'By_1hr', 'Bx_1hr', 'AL_1hr', 'AU_1hr',\n",
       "       'SymH_1hr', 'PC_1hr', 'vsw_1hr', 'psw_1hr', 'Bz_45min', 'By_45min',\n",
       "       'Bx_45min', 'AL_45min', 'AU_45min', 'SymH_45min', 'PC_45min',\n",
       "       'vsw_45min', 'psw_45min', 'Bz_30min', 'By_30min', 'Bx_30min',\n",
       "       'AL_30min', 'AU_30min', 'SymH_30min', 'PC_30min', 'vsw_30min',\n",
       "       'psw_30min', 'Bz_10min', 'By_10min', 'Bx_10min', 'AL_10min', 'AU_10min',\n",
       "       'SymH_10min', 'PC_10min', 'vsw_10min', 'psw_10min'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns.drop('ELE_TOTAL_ENERGY_FLUX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "column1 = pd.Series(df.columns.drop('ELE_TOTAL_ENERGY_FLUX'))\n",
    "column2 = pd.Series(fit.support_)\n",
    "column3 = pd.Series(fit.ranking_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df = pd.DataFrame ({ 'Feature': column1,  'Selected': column2, 'Rank': column3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Selected</th>\n",
       "      <th>Rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SC_AACGM_LAT</td>\n",
       "      <td>False</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID_SC</td>\n",
       "      <td>False</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sin_ut</td>\n",
       "      <td>False</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cos_ut</td>\n",
       "      <td>False</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sin_doy</td>\n",
       "      <td>False</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>AU_10min</td>\n",
       "      <td>False</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>SymH_10min</td>\n",
       "      <td>False</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>PC_10min</td>\n",
       "      <td>False</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>vsw_10min</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>psw_10min</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>72 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Feature  Selected  Rank\n",
       "0   SC_AACGM_LAT     False    17\n",
       "1          ID_SC     False    63\n",
       "2         sin_ut     False    10\n",
       "3         cos_ut     False    26\n",
       "4        sin_doy     False     6\n",
       "..           ...       ...   ...\n",
       "67      AU_10min     False    41\n",
       "68    SymH_10min     False    54\n",
       "69      PC_10min     False    29\n",
       "70     vsw_10min     False     4\n",
       "71     psw_10min     False    32\n",
       "\n",
       "[72 rows x 3 columns]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Selected</th>\n",
       "      <th>Rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Bx_6hr</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>AL_6hr</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>vsw_6hr</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>psw_6hr</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Bz_3hr</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>By_3hr</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>psw_1hr</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>By_45min</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>AL_45min</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>Bz_10min</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Feature  Selected  Rank\n",
       "20    Bx_6hr      True     1\n",
       "21    AL_6hr      True     1\n",
       "25   vsw_6hr      True     1\n",
       "26   psw_6hr      True     1\n",
       "27    Bz_3hr      True     1\n",
       "28    By_3hr      True     1\n",
       "44   psw_1hr      True     1\n",
       "46  By_45min      True     1\n",
       "48  AL_45min      True     1\n",
       "63  Bz_10min      True     1"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rslt_df = feature_df.loc[feature_df['Selected'] == True] \n",
    "rslt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = rslt_df['Feature']\n",
    "selected_features = pd.Series(selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bx_6hr',\n",
       " 'AL_6hr',\n",
       " 'vsw_6hr',\n",
       " 'psw_6hr',\n",
       " 'Bz_3hr',\n",
       " 'By_3hr',\n",
       " 'psw_1hr',\n",
       " 'By_45min',\n",
       " 'AL_45min',\n",
       " 'Bz_10min']"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_features\n",
    "feature_cols = list(selected_features)\n",
    "feature_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_features=['Bx_6hr','AL_6hr','vsw_6hr','psw_6hr','Bz_3hr','By_3hr','psw_1hr','By_45min','AL_45min','Bz_10min','ELE_TOTAL_ENERGY_FLUX','ID_SC']\n",
    "\n",
    "\n",
    "df_new = df_tmp[key_features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bx_6hr</th>\n",
       "      <th>AL_6hr</th>\n",
       "      <th>vsw_6hr</th>\n",
       "      <th>psw_6hr</th>\n",
       "      <th>Bz_3hr</th>\n",
       "      <th>By_3hr</th>\n",
       "      <th>psw_1hr</th>\n",
       "      <th>By_45min</th>\n",
       "      <th>AL_45min</th>\n",
       "      <th>Bz_10min</th>\n",
       "      <th>ELE_TOTAL_ENERGY_FLUX</th>\n",
       "      <th>ID_SC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Datetimes</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1987-01-12 12:57:00</th>\n",
       "      <td>-2.877500</td>\n",
       "      <td>-18.750000</td>\n",
       "      <td>337.700000</td>\n",
       "      <td>3.977500</td>\n",
       "      <td>3.435000</td>\n",
       "      <td>9.126667</td>\n",
       "      <td>4.765000</td>\n",
       "      <td>6.06</td>\n",
       "      <td>-107.0</td>\n",
       "      <td>1.25</td>\n",
       "      <td>2.057827e+08</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1987-01-12 12:58:00</th>\n",
       "      <td>-2.877500</td>\n",
       "      <td>-18.750000</td>\n",
       "      <td>337.700000</td>\n",
       "      <td>3.977500</td>\n",
       "      <td>3.435000</td>\n",
       "      <td>9.126667</td>\n",
       "      <td>4.765000</td>\n",
       "      <td>6.06</td>\n",
       "      <td>-107.0</td>\n",
       "      <td>1.25</td>\n",
       "      <td>2.509837e+08</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1987-01-12 13:01:00</th>\n",
       "      <td>-2.364167</td>\n",
       "      <td>-14.500000</td>\n",
       "      <td>338.100000</td>\n",
       "      <td>4.078333</td>\n",
       "      <td>2.738333</td>\n",
       "      <td>9.205000</td>\n",
       "      <td>4.770000</td>\n",
       "      <td>5.58</td>\n",
       "      <td>-92.0</td>\n",
       "      <td>1.51</td>\n",
       "      <td>4.469146e+08</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1987-01-12 13:02:00</th>\n",
       "      <td>-2.364167</td>\n",
       "      <td>-14.500000</td>\n",
       "      <td>338.100000</td>\n",
       "      <td>4.078333</td>\n",
       "      <td>2.738333</td>\n",
       "      <td>9.205000</td>\n",
       "      <td>4.770000</td>\n",
       "      <td>5.58</td>\n",
       "      <td>-92.0</td>\n",
       "      <td>1.51</td>\n",
       "      <td>1.934849e+10</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1987-01-12 13:03:00</th>\n",
       "      <td>-2.364167</td>\n",
       "      <td>-14.500000</td>\n",
       "      <td>338.100000</td>\n",
       "      <td>4.078333</td>\n",
       "      <td>2.738333</td>\n",
       "      <td>9.205000</td>\n",
       "      <td>4.770000</td>\n",
       "      <td>5.58</td>\n",
       "      <td>-92.0</td>\n",
       "      <td>1.51</td>\n",
       "      <td>3.913337e+11</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1987-01-12 13:04:00</th>\n",
       "      <td>-2.364167</td>\n",
       "      <td>-14.500000</td>\n",
       "      <td>338.100000</td>\n",
       "      <td>4.078333</td>\n",
       "      <td>2.738333</td>\n",
       "      <td>9.205000</td>\n",
       "      <td>4.770000</td>\n",
       "      <td>5.58</td>\n",
       "      <td>-92.0</td>\n",
       "      <td>1.51</td>\n",
       "      <td>8.838582e+10</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1987-01-28 19:08:00</th>\n",
       "      <td>1.524167</td>\n",
       "      <td>-120.333333</td>\n",
       "      <td>438.308333</td>\n",
       "      <td>3.158333</td>\n",
       "      <td>-2.400000</td>\n",
       "      <td>-3.436667</td>\n",
       "      <td>3.278333</td>\n",
       "      <td>3.10</td>\n",
       "      <td>-455.0</td>\n",
       "      <td>6.67</td>\n",
       "      <td>6.824302e+07</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1987-01-28 19:09:00</th>\n",
       "      <td>1.524167</td>\n",
       "      <td>-120.333333</td>\n",
       "      <td>438.308333</td>\n",
       "      <td>3.158333</td>\n",
       "      <td>-2.400000</td>\n",
       "      <td>-3.436667</td>\n",
       "      <td>3.278333</td>\n",
       "      <td>3.10</td>\n",
       "      <td>-455.0</td>\n",
       "      <td>6.67</td>\n",
       "      <td>1.114998e+08</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1987-01-28 19:10:00</th>\n",
       "      <td>1.181667</td>\n",
       "      <td>-118.916667</td>\n",
       "      <td>435.833333</td>\n",
       "      <td>3.231667</td>\n",
       "      <td>-1.720000</td>\n",
       "      <td>-0.915000</td>\n",
       "      <td>3.153333</td>\n",
       "      <td>3.07</td>\n",
       "      <td>-455.0</td>\n",
       "      <td>6.44</td>\n",
       "      <td>1.397239e+08</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1987-01-28 19:11:00</th>\n",
       "      <td>1.181667</td>\n",
       "      <td>-118.916667</td>\n",
       "      <td>435.833333</td>\n",
       "      <td>3.231667</td>\n",
       "      <td>-1.720000</td>\n",
       "      <td>-0.915000</td>\n",
       "      <td>3.153333</td>\n",
       "      <td>3.07</td>\n",
       "      <td>-455.0</td>\n",
       "      <td>6.44</td>\n",
       "      <td>4.325677e+07</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Bx_6hr      AL_6hr     vsw_6hr   psw_6hr    Bz_3hr  \\\n",
       "Datetimes                                                                   \n",
       "1987-01-12 12:57:00 -2.877500  -18.750000  337.700000  3.977500  3.435000   \n",
       "1987-01-12 12:58:00 -2.877500  -18.750000  337.700000  3.977500  3.435000   \n",
       "1987-01-12 13:01:00 -2.364167  -14.500000  338.100000  4.078333  2.738333   \n",
       "1987-01-12 13:02:00 -2.364167  -14.500000  338.100000  4.078333  2.738333   \n",
       "1987-01-12 13:03:00 -2.364167  -14.500000  338.100000  4.078333  2.738333   \n",
       "1987-01-12 13:04:00 -2.364167  -14.500000  338.100000  4.078333  2.738333   \n",
       "1987-01-28 19:08:00  1.524167 -120.333333  438.308333  3.158333 -2.400000   \n",
       "1987-01-28 19:09:00  1.524167 -120.333333  438.308333  3.158333 -2.400000   \n",
       "1987-01-28 19:10:00  1.181667 -118.916667  435.833333  3.231667 -1.720000   \n",
       "1987-01-28 19:11:00  1.181667 -118.916667  435.833333  3.231667 -1.720000   \n",
       "\n",
       "                       By_3hr   psw_1hr  By_45min  AL_45min  Bz_10min  \\\n",
       "Datetimes                                                               \n",
       "1987-01-12 12:57:00  9.126667  4.765000      6.06    -107.0      1.25   \n",
       "1987-01-12 12:58:00  9.126667  4.765000      6.06    -107.0      1.25   \n",
       "1987-01-12 13:01:00  9.205000  4.770000      5.58     -92.0      1.51   \n",
       "1987-01-12 13:02:00  9.205000  4.770000      5.58     -92.0      1.51   \n",
       "1987-01-12 13:03:00  9.205000  4.770000      5.58     -92.0      1.51   \n",
       "1987-01-12 13:04:00  9.205000  4.770000      5.58     -92.0      1.51   \n",
       "1987-01-28 19:08:00 -3.436667  3.278333      3.10    -455.0      6.67   \n",
       "1987-01-28 19:09:00 -3.436667  3.278333      3.10    -455.0      6.67   \n",
       "1987-01-28 19:10:00 -0.915000  3.153333      3.07    -455.0      6.44   \n",
       "1987-01-28 19:11:00 -0.915000  3.153333      3.07    -455.0      6.44   \n",
       "\n",
       "                     ELE_TOTAL_ENERGY_FLUX  ID_SC  \n",
       "Datetimes                                          \n",
       "1987-01-12 12:57:00           2.057827e+08    6.0  \n",
       "1987-01-12 12:58:00           2.509837e+08    6.0  \n",
       "1987-01-12 13:01:00           4.469146e+08    6.0  \n",
       "1987-01-12 13:02:00           1.934849e+10    6.0  \n",
       "1987-01-12 13:03:00           3.913337e+11    6.0  \n",
       "1987-01-12 13:04:00           8.838582e+10    6.0  \n",
       "1987-01-28 19:08:00           6.824302e+07    6.0  \n",
       "1987-01-28 19:09:00           1.114998e+08    6.0  \n",
       "1987-01-28 19:10:00           1.397239e+08    6.0  \n",
       "1987-01-28 19:11:00           4.325677e+07    6.0  "
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateAndTrainOptModel():\n",
    "    # Defining Checkpoints fr the model \n",
    "    checkpoint_name = 'Optimal-Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \n",
    "    file_name = os.path.join(\"./model/\",checkpoint_name)\n",
    "    checkpoint = ModelCheckpoint(file_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n",
    "    callback_ES = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "    callback_TB = tf.keras.callbacks.TensorBoard(log_dir='./logs')\n",
    "    callbacks_list = [checkpoint,callback_ES,callback_TB]\n",
    "\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(20, input_dim=10, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_error', optimizer='sgd')\n",
    "    #estimators = []\n",
    "    #estimators.append(('standardize', StandardScaler()))\n",
    "    #estimators.append(('mlp', KerasRegressor(build_fn=wider_model, epochs=100, batch_size=5, verbose=0, callbacks=callbacks_list)))\n",
    "    #pipeline = Pipeline(estimators)\n",
    "    #return pipeline\n",
    "    history = model.fit(X_train_scaled,y_train_log, epochs=100,validation_data=(X_val_scaled,y_val_log),callbacks=callbacks_list)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Bx_6hr', 'AL_6hr', 'vsw_6hr', 'psw_6hr', 'Bz_3hr', 'By_3hr', 'psw_1hr',\n",
       "       'By_45min', 'AL_45min', 'Bz_10min', 'ELE_TOTAL_ENERGY_FLUX', 'ID_SC'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... dropping rows where total electron energy flux is greater than 1.2880E+13 [eV/cm2/s/ster]\n",
      "--------> number dropped = 98\n",
      "(1945691, 12)\n",
      "validation data shape = (55208, 12)\n",
      "train data shape = (1836963, 12)\n",
      "NOTE: we have called the withheld data *validation* data here\n",
      "NOTE: removing the steradian from the DMSP data BEFORE training\n",
      "Epoch 1/100\n",
      "57389/57406 [============================>.] - ETA: 0s - loss: 75.7856\n",
      "Epoch 1: val_loss improved from inf to 71.51794, saving model to ./model\\Optimal-Weights-001--71.51794.hdf5\n",
      "57406/57406 [==============================] - 125s 2ms/step - loss: 75.7853 - val_loss: 71.5179\n",
      "Epoch 2/100\n",
      "57397/57406 [============================>.] - ETA: 0s - loss: 75.7785\n",
      "Epoch 2: val_loss improved from 71.51794 to 71.51785, saving model to ./model\\Optimal-Weights-002--71.51785.hdf5\n",
      "57406/57406 [==============================] - 126s 2ms/step - loss: 75.7784 - val_loss: 71.5179\n",
      "Epoch 3/100\n",
      "57393/57406 [============================>.] - ETA: 0s - loss: 75.7778\n",
      "Epoch 3: val_loss improved from 71.51785 to 71.51784, saving model to ./model\\Optimal-Weights-003--71.51784.hdf5\n",
      "57406/57406 [==============================] - 126s 2ms/step - loss: 75.7777 - val_loss: 71.5178\n",
      "Epoch 4/100\n",
      "57396/57406 [============================>.] - ETA: 0s - loss: 75.7785\n",
      "Epoch 4: val_loss improved from 71.51784 to 71.51782, saving model to ./model\\Optimal-Weights-004--71.51782.hdf5\n",
      "57406/57406 [==============================] - 128s 2ms/step - loss: 75.7782 - val_loss: 71.5178\n",
      "Epoch 5/100\n",
      "57392/57406 [============================>.] - ETA: 0s - loss: 75.7780\n",
      "Epoch 5: val_loss did not improve from 71.51782\n",
      "57406/57406 [==============================] - 128s 2ms/step - loss: 75.7780 - val_loss: 71.5178\n",
      "Epoch 6/100\n",
      "57403/57406 [============================>.] - ETA: 0s - loss: 75.7781\n",
      "Epoch 6: val_loss improved from 71.51782 to 71.51781, saving model to ./model\\Optimal-Weights-006--71.51781.hdf5\n",
      "57406/57406 [==============================] - 132s 2ms/step - loss: 75.7781 - val_loss: 71.5178\n",
      "1726/1726 [==============================] - 3s 2ms/step - loss: 71.5178\n",
      "71.51780700683594\n",
      "Evaluation: \n",
      "\n",
      "   MAE  = 8.317060409697588\n",
      "   MSE  = 71.51790848544867\n",
      "   RMSE = 8.456826147287686\n",
      "   RAE  = 6.009720595349688\n",
      "   RSE  = 30.505655208371504\n",
      "   RSQ  = -29.505655208371504\n"
     ]
    }
   ],
   "source": [
    "# Creating a simple model with 10 features\n",
    "\n",
    "X_train_scaled, X_val_scaled,y_train_log, y_val_log = PreProcessData(df_new)\n",
    "# Create a model\n",
    "model1 = CreateAndTrainOptModel()\n",
    "\n",
    "# Train the model\n",
    "y_preds = model1.predict(X_val_scaled)\n",
    "\n",
    "mse_test = model1.evaluate(X_val_scaled,y_val_log)\n",
    "print(mse_test)\n",
    "\n",
    "actual = np.array(y_val_log)\n",
    "predicted = y_preds.reshape(55208,)\n",
    "\n",
    "ValidateModel(actual,predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for plotting feature importance\n",
    "def plot_features(columns, importances, n=20):\n",
    "    df = (pd.DataFrame({\"features\": columns,\n",
    "                        \"feature_importance\": importances})\n",
    "          .sort_values(\"feature_importance\", ascending=False)\n",
    "          .reset_index(drop=True))\n",
    "    \n",
    "    sns.barplot(x=\"feature_importance\",\n",
    "                y=\"features\",\n",
    "                data=df[:n],\n",
    "                orient=\"h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_features(X.columns, fs.scores_)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "466c01029850281e301a495ae9b6270c24d2f4c9c212c2321fbac3dbd25eb05a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
